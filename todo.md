# potential todos
- learning multivariate Gaussians for classification
  * found in intro to ML HW1
- impact of PCA/SVD on KNN performance
  * You could apply SVD and PCA, which are very similar, to a simple KNN problem. You used PCA in intro to ML HW2 and SVD in Data Mining HW1. Both involved KNN classification in conjunction with dimensionality reduction. Here's a nice stack overflow showing how the methods are related: https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca
- K Means clustering
  * There are lots of objective functions to use in K means, it's not always Euclidean distance. See your data mining assignment
- EM algorithm and how it relates to clustering and image compression
  * found in intro to ML HW3
- MLP (multilayer perceptron aka neural net) for digit classification. Maybe even explore convolutional net and use TensorFlow from AI 2
  * found in intro to ML HW4. Tensorflow stuff found in AI II HW 4
- use a package (scikit learn maybe) to show how a SVM works
  * intro to ML HW5 compares SVM to the kernel perceptron (both are discriminative classifiers)
- Basic Bayes Nets stuff. Main idea is that they can represent a full joint distribution with the conditional assumptions between parent nodes and child nodes
  * chapter 14 of AIMA
- Mario Party Q Learning
  * project for AI II. Just show how you represent the problem as a state space and then use RL to find an optimal policy
- Decision Trees
  * found in AI II HW 3-4
- Pattern mining from transaction data, i.e., frequent itemset mining and then high confidence rule mining
  * found in Data Mining HW 1
- Various recommendation algorithms from Recommender Systems course
- Given a dataset, how to compare the fits of various statisitcal distributions (e.g. Gaussian vs. Gamma)
  * From Theory of Statistics II
